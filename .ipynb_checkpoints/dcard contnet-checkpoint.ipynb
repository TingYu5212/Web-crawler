{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#參考網站\n",
    "#https://ithelp.ithome.com.tw/articles/10203216\n",
    "#https://tlyu0419.github.io/2019/04/06/Crawl-Dcard/\n",
    "#https://www.dcard.tw/_api/forums/pet/posts?popular=false&limit=30&before=233050496\n",
    "\n",
    "import urllib.request as req\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import gettext\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請問您想選擇哪一版？pet\n",
      "Dcard伺服器狀態:連線中\n",
      "It cost 4.470276 sec\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "ua = UserAgent()\n",
    "uar = ua.random\n",
    "chose_theme = str(input(\"請問您想選擇哪一版？\"))\n",
    "url=\"https://www.dcard.tw/f/\"+chose_theme+\"/?latest=true\"\n",
    "#url=\"https://www.dcard.tw/f/pet\"\n",
    "p = requests.Session()\n",
    "request=req.Request(url, headers={\n",
    "    \"User-Agent\":\"uaa\"\n",
    "})\n",
    "resp = requests.get(url) #回傳為一個request.Response的物件\n",
    "if(int(resp.status_code)==200):\n",
    "    print(\"Dcard伺服器狀態:連線中\")\n",
    "else:\n",
    "    print(\"Dcard伺服器狀態:失敗\")\n",
    "    os.system(\"pause\")\n",
    "    os._exit()\n",
    "html = resp.content.decode('utf-8')\n",
    "soup = BeautifulSoup(resp.text,\"lxml\")\n",
    "\n",
    "tEnd = time.time()\n",
    "print(\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It cost 57.645590 sec\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "sel = soup.find_all('a', class_=\"PostEntry_root_V6g0rd\")\n",
    "a=[]\n",
    "for s in sel:\n",
    "    a.append(s[\"href\"])\n",
    "    x=\"https://www.dcard.tw\"+s[\"href\"]\n",
    "    #link = \"https://www.dcard.tw/f/relationship/p/233043459\"\n",
    "    requ = requests.get(x)\n",
    "    soup1 = BeautifulSoup(requ.text,\"lxml\")\n",
    "    title = soup1.find('h1', class_=\"Post_title_2O-1el\").text\n",
    "    content = soup1.select('.Post_content_NKEl9d div div div')\n",
    "    \n",
    "    with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "        file.write(\"標題:\"+title+\"\\n\")\n",
    "    for tag in content:\n",
    "        tagg=str(tag) \n",
    "        with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "            file.write(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\")+\"\\n\") \n",
    " \n",
    "        \n",
    "tEnd = time.time()\n",
    "print(\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It cost 212.223935 sec\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "for k in range(2):\n",
    "        post_data={\n",
    "            \"before\":a[-1][9:18],\n",
    "            \"limit\":\"30\",\n",
    "            \"popular\":\"true\"\n",
    "        }\n",
    "        r = p.get(\"https://www.dcard.tw/_api/forums/pet/posts\",params=post_data, headers = { \"Referer\": \"https://www.dcard.tw/\", \"User-Agent\": \"uaa\" })\n",
    "        rr=\"https://www.dcard.tw/_api/forums/\"+chose_theme+\"/posts?popular=false&limit=30&before=\"+a[-1][9:18]\n",
    "        resq = requests.get(rr)\n",
    "        data2 = json.loads(resq.text)\n",
    "        for u in range(len(data2)):\n",
    "            Temporary_url = \"/f/pet/p/\"+ str(data2[u][\"id\"]) + \"-\" + str(data2[u][\"title\"].replace(\" \",\"-\"))\n",
    "            conx=\"https://www.dcard.tw\"+Temporary_url\n",
    "            a.append(Temporary_url)\n",
    "            conrequ = requests.get(conx)\n",
    "            soup2 = BeautifulSoup(conrequ.text,\"lxml\")\n",
    "            content2 = soup2.select('.Post_content_NKEl9d div div div')\n",
    "            title2 = soup2.find('h1', class_=\"Post_title_2O-1el\").text\n",
    "            \n",
    "            with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "                file.write(\"標題:\"+title2+\"\\n\")\n",
    "            \n",
    "            for tag in content2:\n",
    "                tagg=str(tag) \n",
    "                with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "                    file.write(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\")+\"\\n\")            \n",
    "\n",
    "tEnd = time.time()\n",
    "print(\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "騙我得A流結果跟小三去旅遊= =\n"
     ]
    }
   ],
   "source": [
    "#標題\n",
    "link = \"https://www.dcard.tw/f/relationship/p/233043459\"\n",
    "requ = requests.get(link)\n",
    "soup1 = BeautifulSoup(requ.text,\"lxml\")\n",
    "title = soup1.find('h1', class_=\"Post_title_2O-1el\").text\n",
    "print(title)\n",
    "with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "    file.write(\"標題:\"+title+\"\\n\")\n",
    "with open(\"Comment.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "    file.write(\"標題:\"+title+\"\\n\")    \n",
    "#fileObject1 = open(\"Content.txt\",\"a+\",encoding=\"utf-8\")\n",
    "#fileObject2 = open(\"Comment.txt\",\"a+\",encoding=\"utf-8\")\n",
    "#fileObject.write(title)\n",
    "#fileObject.write(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章\n",
    "link = \"https://www.dcard.tw/f/relationship/p/233043459\"\n",
    "link = \"https://www.dcard.tw/\"+id\n",
    "requ = requests.get(link)\n",
    "s = BeautifulSoup(requ.text,\"lxml\")\n",
    "content = s.select('.Post_content_NKEl9d div div div')\n",
    "print(content)\n",
    "for tag in content:\n",
    "    tagg=str(tag) \n",
    "    with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "        file.write(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\")+\"\\n\")\n",
    "#print(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合好 再甩掉  \n",
      "不過也可以直接分手\n",
      "但一定要分的爽快直接不聯絡\n",
      "也不用對他說教 浪費時間\n",
      "不過阿姨是好人 可惜一個好婆婆\n",
      "B1 阿姨真的對我很好，可惜無緣了。\n",
      "B2 目前他的道歉訊息我完全不讀不回，電話一通也沒接，他媽媽的我倒是有接，畢竟他媽媽還是無辜的。\n",
      "他根本沒想過妳會去他家，想說神不知鬼不覺的，離開他是妳做得最對的決定，說會離開她騙誰啊，那幾天都住在一起怎麼可能什麼都沒發生\n",
      "因武漢肺炎剛好看清一個人啊！\n",
      "也還好媽媽明理\n",
      "合好 再甩掉  \n",
      "不過也可以直接分手\n",
      "但一定要分的爽快直接不聯絡\n",
      "也不用對他說教 浪費時間\n",
      "不過阿姨是好人 可惜一個好婆婆\n",
      "至少現在看清還不算晚\n",
      "B1 阿姨真的對我很好，可惜無緣了。\n",
      "B2 目前他的道歉訊息我完全不讀不回，電話一通也沒接，他媽媽的我倒是有接，畢竟他媽媽還是無辜的。\n",
      "B3 真的，內心意外很平靜。\n",
      "B4 可惜一個好婆婆\n",
      "阿姨是個好人\n",
      "可惜有個沒用的渣兒子\n",
      "他根本沒想過妳會去他家，想說神不知鬼不覺的，離開他是妳做得最對的決定，說會離開她騙誰啊，那幾天都住在一起怎麼可能什麼都沒發生\n",
      "好八點檔的劇情喔\n",
      "讚讚 男友媽還行\n",
      "心情平靜感覺過幾天哭一場排毒就可以痊癒了\n",
      "記得去做個spa然後找朋友快樂去！\n",
      "喔當然還有注意安全顧好身體\n",
      "這男的超爛的啦\n",
      "噁心以為不會被發現\n",
      "離開的好！！！拜託不要心軟回去復合\n",
      "林采緹曾說過一段我覺得很有道理的話：\n",
      "「為什麼出軌不該原諒？因為我覺得每個人都值得遇到那個『因為愛你』所以不願意去做出任何會失去你的風險的人，他今天會選擇傷害你，就是他有僥倖心態，我覺得你如果原諒的話，痛苦的是自己，這是我的真實經驗。」\n",
      "超扯的啦 爛男人\n",
      "我前任也騙我拉肚子\n",
      "事實上和前任去跨年\n",
      "超爛！\n",
      "跨年還是我生日^_^\n",
      "B13 摩羯！\n",
      "B14 會看星盤？😂\n",
      "B15 沒有沒有！\n",
      "只是有朋友剛好那幾天生日！特別記\n",
      "男女一樣噁\n",
      "還好有發現QQ\n",
      "恭喜你！!\n",
      "提早發現，沒有讓他繼續耽誤你的人生\n",
      "抱一個 祝你好好的\n",
      "妳好帥 擺脫爛桃花更好的要來了\n",
      "祝你遇到一個可以修成正果的人💪\n",
      "泥馬的死渣男很會演戲\n",
      "渣男，還好還沒結婚。\n",
      "再演啊幹 都不要回他！讓他焦急死！\n",
      "B1 有明理嗎🤣🤣 明理的話不會叫他復合\n",
      "還好他有一點點良心是詛咒自己生病，而不是說什麼媽媽生病誰誰過世要幫忙ಠ╭╮ಠ\n",
      "我好奇後面會怎麼處理\n",
      "B26 目前我就完全不讀不回也不接電話啦，可是我還沒封鎖他，好想看他什麼時候會放棄^^\n",
      "B24 至少不是認為自己兒子都是對的啊\n",
      "千萬不要心軟復合\n",
      "但真的可惜了 他媽媽感覺人很好\n",
      "有的家庭還會幫忙隱瞞的⋯像我前男友⋯\n",
      "利用對方的擔心來這樣騙...\n",
      "好心疼原po...辛苦了......\n",
      "不會很氣憤他們憑什麼好好的在一起嗎？\n",
      "那麼喜歡他的自己又算什麼呢？比不上一個明知有女友還硬要湊過來的人...\n",
      "即時知道跟自己無關了，但還是好不甘心...\n",
      "該怎麼辦呢？我就一直卡在這...所以想請教原po...\n",
      "B4 「也請我跟他復合之類的」\n",
      "他媽也是垃圾吧？不然就是可以接受老公外遇\n",
      "有夠蠢的 甩的好\n",
      "B31 有點激動了 \n",
      "我想可能只是他兒子拜託他媽勸說而已 \n",
      "如果今天他媽知情且一同欺騙原po那麼再來罵也不遲\n",
      "女表子配狗，天長地久\n",
      "傻眼\n",
      "有押韻欸\n",
      "渣男 分得好!!\n",
      "祝原po遇到值得妳的人\n",
      "B30 我心裡也很痛苦，可是不知道為什麼可能是因為真的被劈腿的緣故現在對他是厭惡大於痛苦，尤其他現在這樣瘋狂道歉解釋讓人更覺得噁心。\n",
      "我只覺得他們的感情一定不可能會善終啦，這種偷來暗去的感情怎麼會穩固。\n",
      "而且我自己是認為真的非常不值得為了一個爛人讓自己這麼痛苦，人生還有很多有意義的事要做，要讀書、要工作、友情也很重要，真的不必要為了一個根本不在乎自己的人這麼難過。\n",
      "你也加油，我們一起走過！\n",
      "阿姨對你真好，要揭穿自己兒子。100個可能才1個。\n",
      "阿姨表示：抱歉我沒教他偷吃嘴要擦乾淨\n",
      "恭喜你，做了一個對的決定\n",
      "恭喜看清不用浪費時間在他身上\n",
      "到底為什麼一些拉基渣男劈腿劈得毫不猶豫、豪不愧疚，等正宮發現了又瘋狂道歉、瘋狂挽回？？？\n",
      "真的重要的話就不要劈腿啊，腦子是撞到嗎？\n",
      "媽媽還跟著求你回去複合，感覺心態不行，自己兒子那麼爛還好意思叫你再給爛貨一個機會，也是一個道理跟兒子選，一樣選兒子的母親ㄏ\n",
      "渣男\n",
      "加油～～幸好發現 希望你會遇到更好的\n",
      "笑死 還有一天 斷屁斷 哈哈哈哈哈 這個真的太好笑了\n",
      "垃圾\n",
      "拜託他們結婚，我想看還沒拜碼頭就騙準婆婆的逆媳之後被虐二十年的報應\n",
      "祝渣男婊子直接得到重度A流~~~^0^\n",
      "所以那女的很婊耶⋯知道他有女朋友⋯還這樣⋯\n"
     ]
    }
   ],
   "source": [
    "#留言,目前只能用一頁.無法區分留言者 \n",
    "link = \"https://www.dcard.tw/f/relationship/p/233043459\"\n",
    "requ = requests.get(link)\n",
    "soup1 = BeautifulSoup(requ.text,\"lxml\")\n",
    "sel2 = soup1.select('.CommentEntry_content_1ATrw1 div div div ')\n",
    "for tag in sel2:\n",
    "    tagg=str(tag)\n",
    "    with open(\"Comment.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "        file.write(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\").replace(\"<span>\" , \"\").replace(\"</span>\" , \"\").replace(\"<a>\" , \"\").replace(\"</a>\" , \"\")+\"\\n\")\n",
    "    #print(tagg.replace(\"<div>\" , \"\").replace(\"</div>\", \"\").replace(\"<span>\" , \"\").replace(\"</span>\" , \"\").replace(\"<a>\" , \"\").replace(\"</a>\" , \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.close()\n",
    "print(\"爬蟲結束\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d3cf70bcd5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"標題:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mCrawltitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f/pet/p/233045168-我家養了四種寵物\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-d3cf70bcd5fb>\u001b[0m in \u001b[0;36mCrawltitle\u001b[1;34m(ID)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"標題:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Comment.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"標題:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "def Crawltitle(ID):\n",
    "    link = \"https://www.dcard.tw/\"+ID\n",
    "    requ = requests.get(link)\n",
    "    so = BeautifulSoup(requ.text,\"lxml\")\n",
    "   # title = so.find(\"h1\", class_=\"Post_title_2O-1el\").text\n",
    "    title = so.find(\".Post_title_2O-1el\")→None\n",
    "    print(title)\n",
    "    with open(\"Content.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "        file.write(\"標題:\"+title+\"\\n\")\n",
    "    with open(\"Comment.txt\",\"a+\",encoding=\"utf-8\") as file:    \n",
    "        file.write(\"標題:\"+title+\"\\n\")   \n",
    "\n",
    "Crawltitle(\"f/pet/p/233045168-我家養了四種寵物\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0 #為了印頁數\n",
    "q=0 #為了印張數\n",
    "for i in a[2:]:\n",
    "    url = \"https://www.dcard.tw\"+i\n",
    "    j+=1\n",
    "    print (\"第\",j,\"頁的URL為:\"+url)\n",
    "    #file.write(\"temperature is {} wet is {}%\\n\".format(temperature, humidity))\n",
    "    test.write(\"第 {} 頁的URL為: {} \\n\".format(j,url))\n",
    "    url=requests.get(url)\n",
    "    soup = BeautifulSoup(url.text,\"html.parser\")\n",
    "    sel_jpg = soup.select(\"div.Post_content_NKEl9 div div div img.GalleryImage_image_3lGzO\")\n",
    "    for c in sel_jpg:\n",
    "        q+=1\n",
    "        print(\"第\",q,\"張:\",c[\"src\"])\n",
    "        test.write(\"%\\n\"\"第 {} 張: {} \\n\".format(q,c[\"src\"])) \n",
    "        pic=requests.get(c[\"src\"])\n",
    "        img2 = pic.content\n",
    "        pic_out = open(\"spider/pet/\"+str(q)+\".png\",'wb')\n",
    "        pic_out.write(img2)\n",
    "        pic_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dcard.tw/_api/posts?popular=true&limit=100'\n",
    "resq = requests.get(url)\n",
    "rejs = resq.json()\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(rejs)):\n",
    "    df = df.append(Crawl(rejs[i]['id']),ignore_index=True)\n",
    "print(df.shape)\n",
    "df\n",
    "\n",
    "# 透過迴圈讀取10*100篇文章，若需讀取更多資料，可以將range(10)中的數值提升\n",
    "for j in range(10):\n",
    "    last = str(int(df.tail(1).ID)) # 找出爬出資料的最後一筆ID\n",
    "    url = 'https://www.dcard.tw/_api/posts?popular=true&limit=100&before=' + last\n",
    "    resq = requests.get(url)\n",
    "    rejs = resq.json()\n",
    "    for i in range(len(rejs)):\n",
    "        df = df.append(Crawl(rejs[i]['id']), ignore_index=True)\n",
    "print(df.shape)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
